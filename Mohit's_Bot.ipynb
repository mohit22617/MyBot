{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ThR7OTce5Ba",
        "outputId": "c5a796f8-4e14-4257-d728-d2f5f149530a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 25.2 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\mohit\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n",
            "c:\\Users\\mohit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\mohit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mohit\\.cache\\huggingface\\hub\\models--facebook--blenderbot-1B-distill. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "\nAutoModelForSeq2SeqLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSeq2SeqLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/blenderbot-1B-distill\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchatbot_response\u001b[39m(user_input):\n\u001b[0;32m     11\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(user_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\mohit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:2132\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 2132\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mohit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:2101\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 2101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
            "\u001b[1;31mImportError\u001b[0m: \nAutoModelForSeq2SeqLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSeq2SeqLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece --quiet\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_name = \"facebook/blenderbot-1B-distill\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
        "    reply_ids = model.generate(**inputs)\n",
        "    return tokenizer.decode(reply_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Chat loop\n",
        "print(\"Mohit: Hello! Iâ€™m here to talk with you. Type 'quit' to exit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "    bot_reply = chatbot_response(user_input)\n",
        "    print(f\"Chatbot: {bot_reply}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_vv61VlnI6C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "bouHcu1we88B",
        "outputId": "7ca26ac9-ff9b-48bb-d391-3604b7a97e80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3414834505.py:26: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4a2b38aa1757947061.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4a2b38aa1757947061.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers sentencepiece gradio --quiet\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_name = \"facebook/blenderbot-1B-distill\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Chat function\n",
        "def chatbot_response(user_input, history=[]):\n",
        "    inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
        "    reply_ids = model.generate(**inputs, max_length=200)\n",
        "    bot_reply = tokenizer.decode(reply_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append to chat history\n",
        "    history.append((\"You: \" + user_input, \"Bot: \" + bot_reply))\n",
        "    return history, \"\"  # returning \"\" clears the textbox\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<h2>ðŸ¤– Mohit's  ChatBot</h2>\")\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Type your message here...\")\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    msg.submit(chatbot_response, [msg, chatbot], [chatbot, msg])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Launch the app\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV-4olbV1A22"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8681dd8",
        "outputId": "8fafaa21-e45d-4f18-a057-b5384b7a8402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# Install required libraries\n",
        "# !pip install transformers sentencepiece gradio --quiet # This is not needed in the app.py file for deployment\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_name = \"facebook/blenderbot-1B-distill\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Chat function\n",
        "def chatbot_response(user_input, history=[]):\n",
        "    inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
        "    reply_ids = model.generate(**inputs, max_length=200)\n",
        "    bot_reply = tokenizer.decode(reply_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Append to chat history\n",
        "    history.append((\"You: \" + user_input, \"Bot: \" + bot_reply))\n",
        "    return history, \"\"  # returning \"\" clears the textbox\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<h2>ðŸ¤– Mohit's  ChatBot</h2>\")\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Type your message here...\")\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    msg.submit(chatbot_response, [msg, chatbot], [chatbot, msg])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28dcce30"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the `requirements.txt` file listing the necessary Python libraries for the application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087fd41c",
        "outputId": "f4faf6bf-1157-4caa-d499-89ae35e1aa80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "transformers\n",
        "sentencepiece\n",
        "gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STks40Ni6nxd"
      },
      "outputs": [],
      "source": [
        "# Save your Gradio app code into a file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return f\"Hello {name}!\"\n",
        "\n",
        "iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch()\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LyGZC75x8Lf0"
      },
      "outputs": [],
      "source": [
        "!git config --global user.name \"https://github.com/mohit22617\"\n",
        "!git config --global user.email \"mohitwali70@gmail.com\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC3Hu2iq9LlX",
        "outputId": "e082d5c4-46f9-46d3-c0bd-4f034a5b14e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error: remote origin already exists.\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "!git remote add origin https://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@github.com/mohit22617/MyBot.git\n",
        "!git branch -M main\n",
        "!git push -u origin main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLv-LL759r9h",
        "outputId": "143eff71-75d4-4123-a214-5f7047733bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n",
            "[main bd6293b] first commit\n",
            " 1 file changed, 1 insertion(+)\n",
            "error: remote origin already exists.\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "!echo \"# MyBot\" >> README.md\n",
        "!git init\n",
        "!git add README.md\n",
        "!git commit -m \"first commit\"\n",
        "!git branch -M main\n",
        "!git remote add origin https://github.com/mohit22617/MyBot.git\n",
        "!git push -u origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qyJjy3-n_z0a"
      },
      "outputs": [],
      "source": [
        "!git remote remove origin\n",
        "!git remote add origin https://github.com/mohit22617/MyBot.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xgKQCOVT_0NM",
        "outputId": "506365eb-fada-4a54-bfea-992e81abcb6a"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1004493049.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1004493049.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://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@github.com/mohit22617/MyBot.git\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "https://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@github.com/mohit22617/MyBot.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUn54YJRACnz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed26d6cb"
      },
      "source": [
        "https://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@github.com/mohit22617/MyBot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJU0NgdlAKAX",
        "outputId": "fbaf9ff7-29f8-4388-ffed-9395a81239fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main 6a19ff5] First commit from Colab\n",
            " 24 files changed, 51067 insertions(+)\n",
            " create mode 100644 .config/.last_opt_in_prompt.yaml\n",
            " create mode 100644 .config/.last_survey_prompt.yaml\n",
            " create mode 100644 .config/.last_update_check.json\n",
            " create mode 100644 .config/active_config\n",
            " create mode 100644 .config/config_sentinel\n",
            " create mode 100644 .config/configurations/config_default\n",
            " create mode 100644 .config/default_configs.db\n",
            " create mode 100644 .config/gce\n",
            " create mode 100644 .config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            " create mode 100644 .config/logs/2025.08.11/13.35.02.204179.log\n",
            " create mode 100644 .config/logs/2025.08.11/13.35.30.203988.log\n",
            " create mode 100644 .config/logs/2025.08.11/13.35.39.125256.log\n",
            " create mode 100644 .config/logs/2025.08.11/13.35.45.017566.log\n",
            " create mode 100644 .config/logs/2025.08.11/13.35.54.455059.log\n",
            " create mode 100644 .config/logs/2025.08.11/13.35.55.205921.log\n",
            " create mode 100644 .gradio/certificate.pem\n",
            " create mode 100644 app.py\n",
            " create mode 100644 requirements.txt\n",
            " create mode 100755 sample_data/README.md\n",
            " create mode 100755 sample_data/anscombe.json\n",
            " create mode 100644 sample_data/california_housing_test.csv\n",
            " create mode 100644 sample_data/california_housing_train.csv\n",
            " create mode 100644 sample_data/mnist_test.csv\n",
            " create mode 100644 sample_data/mnist_train_small.csv\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "!git add .\n",
        "!git commit -m \"First commit from Colab\"\n",
        "!git push -u origin main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsJCxLclAKd6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
